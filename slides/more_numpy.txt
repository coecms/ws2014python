Performance NumPy
=================

The Zen of NumPy

* Data arrays are better than objects.

* Contiguous is better than strided.

* Strided is better than scattered.

* Broadcasting is better than reshaping.

* Vectorization is better than looping...

* ... unless it's too complicated; go compile!

(Paraphrased from Travis Oliphant)


Array structure
===============

.. code:: python

   x = np.arange(24).reshape(2, 3, 4)

What do these tell us?

============== ==================
``x.size``     ``(2, 3, 4)``
``x.shape``    ``24``
``x.dtype``    ``dtype('int64')``
``x.itemsize`` ``8``
``x.nbytes``   ``192``
``x.strides``  ``(96, 32, 8)``
============== ==================


Inside the array
================

Look at the data:

.. code:: python

   x = np.array([[1, 2, 3], [4, 5, 6]],
                dtype=np.int8)

Data of any shape is always a sequence:

+----+----+----+----+----+----+
|0x01|0x02|0x03|0x04|0x05|0x06|
+----+----+----+----+----+----+

CPUs work most efficiently along this "vector".


NumPy memory order
==================

Row-major ("C") vs column-major ("Fortran")

.. code:: python

   x = np.array([[1, 2, 3], [4, 5, 6]],
                dtype=np.uint8, order='C')

+----+----+----+----+----+----+
|0x01|0x02|0x03|0x04|0x05|0x06|
+----+----+----+----+----+----+

.. code:: python

   y = np.array([[1, 2, 3], [4, 5, 6]],
                dtype=np.uint8, order='F')

+----+----+----+----+----+----+
|0x01|0x04|0x02|0x05|0x03|0x06|
+----+----+----+----+----+----+


Quiz
====

1. Which is faster?

   * ``np.arange(1e4).sum()``

   * ``np.arange(8e4)[::8].sum()?``

2. Which is faster?

   * ``np.arange(1e4).sum()``

   * ``np.arange(1e4)[::-1].sum()``

3. If ``x.shape`` is ``(10, 10)``, which is contiguous?

   * ``x[:, 3:7]``

   * ``x[3:7, :]``


"Fast" indexing
===============

How do we index something like :math:`u(x, y, z, t)`?

Row-major (``order='C'``):

.. code:: python

   x = np.empty(n_t, n_z, n_y, n_x)

Column-major (``order='F'``):

.. code:: python

   x = np.empty(n_x, n_y, n_z, n_t)

Which variable is contiguous? Which is least so?


Memory Usage
============

How much memory does this use?

.. code:: python

   x = np.empty(N, ...)
   for i in range(N):
      y = f(...)
      x[i] = y


Memory Usage
============

Mem usage: ``x.nbytes + 2 * y.nbytes``

.. code:: python

   x = np.empty(N, ...)
   for i in range(N):
      y = f(...)
      x[i] = y
      del y

``y`` is bound to ``f``, so it's saved until next ``f`` is done.

Use ``del y`` to release early.


Copying Arrays
==============

.. code:: python

   x = np.arange(10)
   y = x
   z = x[:]

What happens to ``y`` and ``z`` under these two operations?

.. code:: python

   x.shape(2,5)
   x[0] = 999.

What does ``x.flags`` say?


How to copy, *really* copy
==========================

Invoke the ``copy`` command:

.. code:: python

   yy = x.copy()

Or pre-allocate and copy the values:

.. code:: python

   zz = np.empty_like(x)
   zz[:] = x


Broadcasting
============

This won't work; outer dimensions don't match

.. code:: python

   x = np.arange(12).reshape(3, 4)
   y = np.arange(12).reshape(4, 3)
   x * y

But this works fine:

.. code:: python

   x = np.arange(12).reshape(3, 4)
   y = np.arange(4)
   x * y


Broadcasting Rules
==================

Outer axes are *broadcast* to inner axes

This is an explicit ``(N,)`` to ``(M, N)`` broadcast:

.. code:: python

   x = np.empty((3,4))
   x[:] = np.arange(4)

This is an implicit ``(1,)``-to-\ ``(N,)`` broadcast:

.. code:: python

   x = np.empty(100)
   x[:] = 5.


Extending dimensions
====================

What if you want to broadcast an inner dimension? Use ``newaxis``:

.. code:: python

   x = np.arange(12).reshape(3, 4)
   y = np.arange(3)

   x * y[:, np.newaxis]

This is another implicit broadcast.


Implicit indexing
=================

You can omit all inner indices with ``...``:

.. code:: python

   x = np.arange(24).reshape(2, 3, 4)

The following two assignments are equivalent:

.. code:: python

   x[:, :, :, 2, 3] = 0.
   x[..., 2, 3] = 0.

Useful when looping over arrays of different shapes.


Exercise
========

Calculate :math:`d/dx \sin x` on :math:`[0, 10)` using a forward difference:

.. math:: \frac{d}{dx} \left(\sin x \right)
            \approx \frac{\sin (x + h) - \sin x}{h}

Use :math:`h = 0.1`


Solution
========

.. code:: python

   x = np.arange(0., 10., 0.1)
   h = x[1:] - x[:-1]

   d_sin = (np.sin(x[1:]) - np.sin(x[:-1])) / h

or just

.. code:: python

   d_sin = np.diff(sin(x)) / np.diff(x)


Exercise
========

Given two random 2D arrays:

.. code:: python

   x = np.random.rand(2, 5)
   y = np.random.rand(3, 5)

vectorise this sum along the tail axis:

.. math:: S_{ij} = \sum_{k} x_{ik} y_{jk}


Solution
========

Two solutions here, one using ``newaxis``:

.. code:: python

   S = (x[:, np.newaxis, :] * y[np.newaxis, :, :]
        ).sum(axis=-1)

Another uses the ``einsum`` function:

.. code:: python

   S = np.einsum('ik,jk->ij', x, y)


Exercise
========

Solve :math:`\nabla^2 \phi = 0` on a 2D square, with

.. math::

   \phi(x, 0) &= 0   &  \phi(x, 1) &= 0 \\
   \phi(0, y) &= 0   &  \phi(1, y) &= \sin \pi x

Use iteration:

.. math::

   \phi_{i+1, j} + \phi_{i-1, j} + \phi_{i, j+1} + \phi_{i, j-1}
      - 4 \phi_{ij} = 0

   \phi^{(n+1)}_{ij} = \frac{1}{4} \left(
                                 \phi^{(n)}_{i+1, j} + \phi^{(n)}_{i-1, j}
                                 + \phi^{(n)}_{i, j+1} + \phi^{(n)}_{i, j-1}
                        \right)


Solution
========

Slow convergence, but it works:

.. code:: python

   a = np.linspace(0., 1., 101)
   x, y = np.meshgrid(a, a)

   p = np.zeros_like(x)
   p[-1, :] = np.sin(np.pi * a)

   for i in xrange(10000):
       p[1:-1, 1:-1] = 0.25 * (  p[:-2, 1:-1]
                               + p[2:, 1:-1]
                               + p[1:-1, :-2]
                               + p[1:-1, 2:] )


Boolean arrays
==============

Set the negative values to zero:

.. code:: python

   x = np.random.randn(10, 10)
   neg_mask = x < 0.
   x[neg_mask] = 0.

Or just save a line

.. code:: python

   x[x < 0] = 0.


Masked Arrays
=============

Exposition here

.. code:: python

   x = np.random.rand(3, 4)
   x_m = np.ma.masked_array(x, x > 0.5)



